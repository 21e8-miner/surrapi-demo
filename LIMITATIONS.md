# SurrAPI â€” Operating Scope & Boundaries

> **We tell you exactly where we work well â€” and where we don't.** 
> This transparency is a feature, not a bug. Most ML tools fail silently.

## Training Domain Boundaries

| Parameter | Training Range | Safe Operating Range | Notes |
|-----------|----------------|---------------------|-------|
| Reynolds (Re) | 500 â€“ 10,000 | 100 â€“ 15,000 | Extrapolation degrades rapidly beyond 15k |
| Angle of Attack (Î±) | -10Â° to +15Â° | -10Â° to +12Â° | Flow separation >12Â° produces unphysical results |
| Mach (M) | 0.1 â€“ 0.4 | 0.1 â€“ 0.3 | M > 0.3 violates incompressible assumption |
| Resolution | 64Â² â€“ 256Â² | 128Â² recommended | Higher resolution = memory hungry |

## Known Failure Modes

### 1. Flow Separation (Critical)
- **Symptom:** Unphysical recirculation bubbles, negative pressure zones
- **Trigger:** Î± > 12Â° or Re > 20,000
- **Detection:** `physics_score < 0.5`, large divergence values
- **Mitigation:** None â€” model is not trained on separated flows

### 2. Shock Capturing (Not Supported)
- **Symptom:** Oscillations near shock locations, non-physical pressure spikes
- **Trigger:** M > 0.6 (transonic regime)
- **Root Cause:** FNO spectral convolutions cannot represent sharp discontinuities
- **Mitigation:** Restrict to subsonic flows (M < 0.3)

### 3. Boundary Layer Resolution
- **Symptom:** Incorrect wall shear stress, underestimated drag
- **Trigger:** High-Re flows on coarse grids
- **Root Cause:** 128Ã—128 grid cannot resolve viscous sublayer (y+ requirements)
- **Mitigation:** Use for qualitative trends only, not quantitative predictions

### 4. Geometry Generalization (Not Implemented)
- **Current Status:** Fixed unit-square domain (lid-driven cavity analog)
- **What Doesn't Work:** Arbitrary airfoil shapes, complex geometries
- **Planned:** Geometry-conditioned training (roadmap item)

### 5. Transient Flows (Not Supported)
- **Symptom:** Static/averaged predictions for inherently unsteady flows
- **Trigger:** Vortex shedding regimes (Re > 40 for cylinders)
- **Root Cause:** Model trained on steady-state solutions only
- **Detection:** Large uncertainty values if `return_uncertainty=True`

## Out-of-Distribution Behavior

When inputs fall outside the training distribution:

1. **Predictions become interpolations of training extremes** (not physics)
2. **physics_score degrades** â€” check response metadata
3. **Divergence increases** â€” âˆ‡Â·u deviates from zero
4. **No explicit warning** is raised (user must check metrics)

### OOD Detection Heuristics

```python
result = client.predict(reynolds=50000, angle=20.0)

# Check 1: Physics score
if result.physics_score < 0.7:
    print("WARNING: Low physics confidence â€” likely OOD")

# Check 2: Request uncertainty quantification
result = client.predict(..., return_uncertainty=True)
if result.mean_uncertainty() > 0.2:
    print("WARNING: High uncertainty â€” predictions unreliable")
```

## Physics Constraints â€” What They Actually Do

### `enforce_conservation=True`

**What it is:**
- Post-processing Helmholtz/Hodge projection onto divergence-free manifold
- Iterative Poisson solver (5 iterations) in spectral space

**What it does:**
- Reduces velocity divergence by ~60-90% on validation set
- Improves `physics_score` metric

**What it does NOT do:**
- âŒ "Guarantee" conservation (projection only corrects predictions, doesn't fix learned representations)
- âŒ Make unphysical predictions physical
- âŒ Work reliably outside training distribution

### `return_uncertainty=True`

**What it is:**
- Monte Carlo dropout with 10 forward passes
- Returns standard deviation across samples

**What it does:**
- Provides variance estimates for each field
- High uncertainty correlates with OOD inputs

**What it does NOT do:**
- âŒ Provide calibrated confidence intervals
- âŒ Guarantee coverage probability
- âŒ Replace proper ensemble methods

## SurrAPI vs Traditional CFD â€” Complementary Tools

| Aspect | SurrAPI | Traditional CFD | Best For |
|--------|---------|-----------------|----------|
| **Speed** | ~300ms | 10min â€“ 10hr | SurrAPI: exploration loops |
| **Accuracy** | 1.5-5% LÂ² | Mesh-converged | CFD: final validation |
| **Cost/run** | ~$0.01 | ~$1-50 | SurrAPI: 1000s of runs |
| **Generalization** | Training domain | Any physics | CFD: novel regimes |
| **Setup time** | Instant | Hours-days | SurrAPI: rapid prototyping |
| **OOD detection** | physics_score warns you | Residual monitoring | Both have safeguards |

**Key insight**: We're not replacing CFD. We're the **1000x cheaper first pass** before you run CFD.

## Best Use Cases

### ðŸš€ SurrAPI Shines Here

| Use Case | Why It Works |
|----------|--------------|
| **Parameter sweeps** | Run 1000 Re/Î± combinations in 5 minutes |
| **Optimization loops** | Sub-second gradients for design search |
| **Real-time dashboards** | Live flow visualization in browser |
| **Sensitivity analysis** | Explore parameter space before CFD |
| **Education/training** | Instant feedback for learning |

### ðŸ”¬ Use CFD Instead

| Situation | Why CFD Is Better |
|-----------|-------------------|
| Final design sign-off | Need mesh-converged accuracy |
| Novel flow regimes | Outside our training domain |
| Safety certification | Regulatory requirements |
| Transonic/supersonic | Shock physics not captured |

## Reporting Issues

If you encounter unexpected behavior:

1. Check if inputs are within training domain (see table above)
2. Enable `return_uncertainty=True` and verify uncertainty levels
3. Compare physics_score to expected range (>0.7 for reasonable predictions)
4. Open an issue with: input parameters, response metrics, expected vs. actual behavior

---

*This document is part of responsible AI disclosure. Last updated: 2026-01-28*
